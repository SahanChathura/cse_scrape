name: Daily CSE Data Scraper
on:
  schedule:
    - cron: '0 14 * * *'  # 9 AM EST daily
  workflow_dispatch:       # Manual trigger option

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      # Step 1: Checkout code with write permissions
      - uses: actions/checkout@v4
        with:
          persist-credentials: true  # Essential for git push
          fetch-depth: 0            # Required for proper git history

      # Step 2: Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      # Step 3: Install dependencies
      - name: Install packages
        run: |
          pip install requests beautifulsoup4 pandas lxml

      # Step 4: Run scraper
      - name: Execute CSE Scraper
        run: python cse_scrape.py

      # Step 5: Commit and push results
      - name: Commit CSV to repository
        run: |
          # Configure git
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"
          
          # Move CSV to dedicated directory (optional)
          mkdir -p cse_data
          mv cse_data.csv cse_data/ 2>/dev/null || echo "No CSV to move"
          
          # Commit changes
          git add cse_data/cse_data.csv
          git commit -m "Automatic CSE data update $(date +'%Y-%m-%d %H:%M')" || echo "No changes to commit"
          git push
