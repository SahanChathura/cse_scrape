name: Daily CSE Data Scraper
on:
  schedule:
    - cron: '0 14 * * *'  # 9 AM EST daily
  workflow_dispatch:       # Manual trigger option

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 10    # Prevents hanging jobs
    
    steps:
      # Step 1: Checkout code
      - uses: actions/checkout@v4
        
      # Step 2: Set up Python
      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      # Step 3: Install dependencies
      - name: Install required packages
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 pandas lxml html5lib
          
          # Install from requirements.txt if it exists
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi
          
      # Step 4: Run scraper with error handling  
      - name: Execute CSE Scraper
        run: |
          {
            python cse_scrape.py 2>&1 | tee scraper.log
          } || {
            echo "Scraper failed with exit code $?"
            exit 0  # Continue workflow despite failure
          }
          
      # Step 5: Upload results
      - name: Archive scraped data
        uses: actions/upload-artifact@v4
        with:
          name: cse-data-${{ github.run_id }}
          path: |
            cse_data.csv
            scraper.log
          retention-days: 7
          
      # Step 6: Notify on failure (optional)
      - name: Notify on failure
        if: failure()
        uses: actions/github-script@v6
        with:
          script: |
            core.warning('Workflow failed! Check ${{ github.run_id }}')
